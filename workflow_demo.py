import asyncio
from langgraph.graph import StateGraph, END
from typing import TypedDict, Annotated
import operator
import asyncpg
import os
from dotenv import load_dotenv
from langchain_core.tools import tool
from langchain_ollama import OllamaEmbeddings
import json
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage
import redis
from redisvl.utils.vectorize import CustomTextVectorizer
from redisvl.extensions.cache.llm import SemanticCache
from utils.database import get_pg_connection, fetchall, fetch_one, get_redis_client

load_dotenv()

llm = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.7,
    api_key=os.getenv("OPENAI_API_KEY")
)

embedd = OllamaEmbeddings(model="bge-m3:latest",
                          base_url="http://localhost:11434")
REDIS_URL = os.getenv("REDIS_URL")
PG_HOST_AI = "localhost"
PG_PORT_AI = 5432
PG_USER = os.getenv("DB_USERNAME")
PG_PASS = os.getenv("DB_PASSWORD")

def embedding(text):
    return embedd.embed_query(text)

class AgentState(TypedDict):
    messages: str
    rag_results: str
    llm_response: str
    final_response: str
    error: str
    
async def search_with_rag(state: AgentState) -> AgentState:
    try:
        question = state['messages']

        question_vector = embedding(question)
        vector_str = '[' + ','.join(map(str, question_vector)) + ']'
        
        pg_pool = await get_pg_connection()
        
        query = """
            SELECT question, answer, bot_type,
            1 - (embedding <=> $1::vector) as similarity
            FROM bot_knowledge
            ORDER BY embedding <=> $1::vector
            LIMIT 3
        """
        
        results = await fetchall(query, vector_str)

        if not results:
            state["rag_results"] = ""
            return state
            
        formatted_results = []
        for row in results:
            formatted_results.append({
                "question": row[0],
                "answer": row[1],
                "similarity": float(row[3])
            })
        
        # ‚úÖ ƒê·ªÉ LLM ch·ªçn c√¢u tr·∫£ l·ªùi t·ªët nh·∫•t
        selection_prompt = f"""D·ª±a v√†o c√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng, h√£y ch·ªçn c√¢u tr·∫£ l·ªùi PH√ô H·ª¢P NH·∫§T t·ª´ c√°c ƒë√°p √°n sau:

C√¢u h·ªèi: {question}

C√°c ƒë√°p √°n:
{json.dumps(formatted_results, ensure_ascii=False, indent=2)}

Ch·ªâ tr·∫£ v·ªÅ JSON v·ªõi format: {{"selected_answer": "c√¢u tr·∫£ l·ªùi ƒë∆∞·ª£c ch·ªçn"}}"""

        messages = [
            SystemMessage(content="B·∫°n l√† chuy√™n gia ph√¢n t√≠ch v√† l·ª±a ch·ªçn th√¥ng tin ch√≠nh x√°c."),
            HumanMessage(content=selection_prompt)
        ]
        
        response = await llm.ainvoke(messages)
        selected = json.loads(response.content)
        
        print("‚úÖ STEP 1: LLM SELECTED ANSWER", selected)
        
        state["rag_results"] = selected["selected_answer"]
        
        return state
    except Exception as e:
        state["rag_results"] = ""
        state["error"] = str(e)
        print(f"Error In RAG :{str(e)}")
        return state
async def ask_llm_node(state: AgentState) -> AgentState:
    """Ask ChatGPT directly"""
    try:
        question = state["messages"]
        messages = [
            SystemMessage(content="B·∫°n l√† tr·ª£ l√Ω ·∫£o c√≥ t√™n g·ªçi l√† Mimi, l√† tr√≠ th√¥ng minh nh√¢n t·∫°o c·ªßa m·ªôt shop h√†ng tr·ª±c tuy·∫øn, b·∫°n s·∫Ω l√† ng∆∞·ªùi c√≥ tr√°ch nhi·ªám h·ªó tr·ª£ customers ƒë·ªÉ t∆∞ v·∫•n v·ªÅ s·∫£n ph·∫©m. S·∫£n ph·∫©m l√† m·ªôt ·ª©ng d·ª•ng website chatbot online tr·ª±c tuy·∫øn"),
            HumanMessage(content=question)
        ]
        response = await llm.ainvoke(messages)
        print("‚úÖ STEP 2: ASK GPT\n", response)
        state["llm_response"] = response.content
        return state
    except Exception as e:
        state["error"] = str(e)
        return state

async def refine_response_node(state: AgentState) -> AgentState:
    """Refine the response for children"""
    try:
        question = state["messages"]
        # Use RAG result if available, otherwise use LLM response
        answer = state.get("rag_results") or state.get("llm_response", "")
        
        messages = [
            SystemMessage(content="""
            H√£y ƒëi·ªÅu ch·ªânh c√¢u tr·∫£ l·ªùi theo y√™u c·∫ßu sau:
            - S·ª≠ d·ª•ng ng√¥n ng·ªØ th·∫≠t ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu, gi·ªçng ƒëi·ªáu nh√≠ nh·∫£nh, ph√π h·ª£p l·ª©a tu·ªïi
            - Tr√°nh thu·∫≠t ng·ªØ ph·ª©c t·∫°p
            - S·ª≠ d·ª•ng c√¢u t·ª´ ng·∫Øn g·ªçn, x√∫c t√≠ch
            - Th√™m c√°c emoji, ƒë·ªÉ t·∫°o s·ª± h·∫•p d·∫´n ƒë·ªëi v·ªõi ng∆∞·ªùi ƒë·ªçc
            - Th√™m v√≠ d·ª• minh h·ªça n·∫øu c·∫ßn
            - Gi·ªØ ng·ªØ ƒëi·ªáu th√¢n thi·ªán, khuy·∫øn kh√≠ch"""),
            HumanMessage(content=f"C√¢u h·ªèi: {question}\n\nC√¢u tr·∫£ l·ªùi c·∫ßn ch·ªânh s·ª≠a{answer}")
        ]
        response = await llm.ainvoke(messages)
        print("‚úÖ STEP 3: REFINE BOT RESPONSE\n", response)
        state["final_response"] = response.content
        return state
    except Exception as e:
        state["error"] = str(e)
        return state
# Conditional edge function
def should_use_llm(state: AgentState) -> str:
    """Decide if we need to call LLM"""
    if state.get("rag_results"):
        return "refine"
    return "ask_llm"

def workflow_demo():
    workflow = StateGraph(AgentState)

    # workflow.add_node("search_rag", search_with_rag)
    workflow.add_node("ask_llm", ask_llm_node)
    workflow.add_node("refine", refine_response_node)

    workflow.set_entry_point("ask_llm")

    # workflow.add_conditional_edges(
    #     "search_rag",
    #     should_use_llm,
    #     {
    #         "ask_llm":"ask_llm",
    #         "refine": "refine" 
    #     }
    # )
    workflow.add_edge("ask_llm", "refine")
    workflow.add_edge("refine", END)

    return workflow.compile()
app = workflow_demo()

async def main():
    """‚úÖ Test workflow - C·∫¶N KH·ªûI T·∫†O POOL TR∆Ø·ªöC"""
    print("üöÄ Starting workflow test...\n")
    
    # ‚úÖ QUAN TR·ªåNG: Kh·ªüi t·∫°o pool tr∆∞·ªõc khi d√πng
    from utils.database import start_pooling, close_db_pools
    await start_pooling()
    
    try:
        while True:
            question = input("Nh·∫≠p c√¢u h·ªèi: ")
            if question == "end":
                break
                
            config = {"configurable": {"thread_id": "user_123"}}
            input_data = {
                'messages': question,
                'rag_results': '',
                'llm_response': '',
                'final_response': '',
                'error': ''
            }
            result = await app.ainvoke(input_data, config=config)
            
            print("\nüì¶ Final Response:", result.get("final_response"))
            
    finally:
        await close_db_pools()

if __name__ == "__main__":
    asyncio.run(main())